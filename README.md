# Detection & Estimation Theory ‚Äî Assignment 4  
**Author:** Ishan Jha  
**Roll Number:** IMT2022562  
**Institution:** IIIT Bangalore  
**Date:** 25 November 2025  

This repository contains the complete solution set for **Assignment 4** of *Detection and Estimation Theory*.  
All problems involve a combination of:

- Mathematical derivations  
- Simulation-driven verification  
- Performance analysis of estimators  
- Use of the Cram√©r‚ÄìRao Lower Bound (CRLB)  

The full written report is available as:

üìÑ **Report.pdf**

---

# üìÅ Repository Structure

Detection_Estimation_Theory_Assignment_4/
‚îÇ
‚îú‚îÄ‚îÄ Q1a.py # Problem 1(a) ‚Äì DC-level estimation with i.i.d. Gaussian noise
‚îú‚îÄ‚îÄ Q1b.py # Problem 1(b) ‚Äì GLS estimation under correlated noise
‚îú‚îÄ‚îÄ Q1c.py # Problem 1(c) ‚Äì Sample mean vs. single-sample estimator
‚îú‚îÄ‚îÄ Q2.py # Problem 2 ‚Äì MLE & CRLB verification for multi-parameter case
‚îÇ
‚îú‚îÄ‚îÄ Report.pdf # Detailed theoretical and simulation write-up
‚îî‚îÄ‚îÄ README.md

# üìò Problem 1: Estimation of a DC Level

## üîπ Problem 1(a): DC Estimation with i.i.d. Gaussian Noise

### **Model**
$$
y[n] = A + w[n], \qquad w[n] \sim \mathcal{N}(0,\sigma^2)
$$

Estimator:
\[
\hat{A} = \frac{1}{N}\sum_{n=0}^{N-1} y[n]
\]

CRLB:
\[
\mathrm{Var}(\hat{A}) \ge \frac{\sigma^2}{N}
\]

### **Functionality of Q1a.py**
- Generates noisy measurements of a constant \(A\)  
- Computes the empirical MSE of \(\hat{A}\)  
- Computes the theoretical CRLB (\(\sigma^2 / N\))  
- Evaluates estimator performance over multiple sample sizes  
- Produces a log‚Äìlog plot comparing MSE and CRLB  

### **Result**
The sample mean reaches the CRLB, confirming it is:
- **Efficient**
- **MVUE**
- The statistically optimal estimator for this i.i.d. scenario.

---

## üîπ Problem 1(b): Estimation with Correlated Gaussian Noise

### **Model**
\[
y[n] = \mu + w[n], \qquad 
\mathbf{w} \sim \mathcal{N}(0,\Sigma)
\]
\[
\Sigma[n,m] = \sigma^2 \rho^{|n-m|}
\]

### **Estimators Compared**
1. **Sample Mean**
   \[
   \hat{\mu}_{SM} = \frac{1}{N}\sum y[n]
   \]

2. **GLS (Generalized Least Squares) Estimator**
   \[
   \hat{\mu}_{GLS}
    = \frac{\mathbf{1}^T \Sigma^{-1} \mathbf{y}}
           {\mathbf{1}^T \Sigma^{-1} \mathbf{1}}
   \]

### **CRLB**
\[
\mathrm{Var}(\hat{\mu}) \ge 
\left( \mathbf{1}^T \Sigma^{-1} \mathbf{1} \right)^{-1}
\]

### **Functionality of Q1b.py**
- Builds a Toeplitz covariance structure  
- Generates correlated Gaussian noise  
- Computes both \(\hat{\mu}_{SM}\) and \(\hat{\mu}_{GLS}\)  
- Computes empirical variances from Monte-Carlo trials  
- Plots histograms of their distributions  
- Compares both MSEs to the CRLB  

### **Result**
- Sample Mean ‚Üí **Not efficient**, does **not** achieve CRLB  
- GLS Estimator ‚Üí **Efficient**, **achieves CRLB**  

The GLS estimator is therefore the MVUE under correlated noise.

---

## üîπ Problem 1(c): Comparison Between Two Unbiased Estimators

This highlights that **‚Äúunbiased‚Äù does not imply ‚Äúoptimal.‚Äù**

### **Two Estimators**
1. **Sample Mean**
   \[
   \hat{A}_{mean} = \frac{1}{N}\sum y[n]
   \]

2. **First Sample**
   \[
   \hat{A}_{single} = y[1]
   \]

### **Variances**
\[
\mathrm{Var}(\hat{A}_{mean}) = \frac{\sigma^2}{N}
\]
\[
\mathrm{Var}(\hat{A}_{single}) = \sigma^2
\]

### **Functionality of Q1c.py**
- Runs many Monte-Carlo trials  
- Computes the empirical variances of both estimators  
- Plots distribution histograms  

### **Conclusion**
- Both estimators are unbiased  
- The single-sample estimator has **much higher variance**  
- Sample mean is **significantly better** and **MVUE** in the i.i.d. case  

# üìò Problem 2: Multi-Parameter Estimation Using the MLE

## üîπ Model Description

In this problem, the observation model is a **linear Gaussian system**:

\[
\mathbf{y} = H\boldsymbol{\theta} + \mathbf{s} + \mathbf{w},
\]

where:

- \( H \) is a known \( N \times p \) observation matrix  
- \( \boldsymbol{\theta} \) is an unknown parameter vector of size \( p \times 1 \)  
- \( \mathbf{s} \) is a known deterministic signal  
- \( \mathbf{w} \sim \mathcal{N}(0, C) \) is Gaussian noise with known covariance \( C \)

This represents a **multiple-parameter estimation** problem in the presence of correlated noise.

---

## üîπ Maximum Likelihood Estimator (MLE)

For the above linear Gaussian model, the log-likelihood is maximized by:

\[
\hat{\boldsymbol{\theta}}_{\mathrm{MLE}}
    = (H^T C^{-1} H)^{-1}
      H^T C^{-1} (\mathbf{y} - \mathbf{s}).
\]

This is also the **Generalized Least Squares (GLS)** estimator and is unbiased.

---

## üîπ Cram√©r‚ÄìRao Lower Bound (CRLB)

For any unbiased estimator of \( \boldsymbol{\theta} \), the covariance must satisfy:

\[
\mathrm{Cov}(\hat{\boldsymbol{\theta}}) 
    \ge (H^T C^{-1} H)^{-1}.
\]

The matrix on the right is the **CRLB matrix**, which sets the minimum achievable variance for each component of the vector parameter.

---

## üîπ What Q2.py Performs

The script carries out a detailed simulation to verify the CRLB for a vector parameter:

### **1. Synthetic Data Generation**
- Randomly generates:
  - True unknown parameter vector \( \boldsymbol{\theta}_{\mathrm{true}} \)
  - Observation matrix \( H \)
  - Covariance matrix \( C \) with controllable correlation structure  

### **2. Computes Theoretical Values**
- Computes the theoretical CRLB matrix  
- Computes the MLE \( \hat{\boldsymbol{\theta}}_{\mathrm{MLE}} \)

### **3. Monte-Carlo Simulation (‚âà 20,000 trials)**
For each trial:
- Generate noise via Cholesky decomposition of \( C \)  
- Create a noisy measurement vector \( \mathbf{y} \)  
- Estimate \( \boldsymbol{\theta} \) using the MLE formula  
- Store the estimate  

### **4. Empirical Performance Analysis**
- Estimate empirical covariance across all trials  
- Compare each variance term to the CRLB diagonal  
- Plot histogram of estimator distribution vs. Gaussian predicted by CRLB  
- Confirm efficiency visually and numerically

---

## üîπ Key Observations and Results

- The empirical variances of the parameter estimates **align extremely well** with the CRLB predictions.  
- This verifies that the **MLE/GLS estimator is efficient** for the Gaussian linear model.  
- In multi-parameter estimation, each component:
  - is unbiased  
  - achieves its individual CRLB value  
- Thus, the estimator is **MVUE** for vector-valued parameters as well.

---

## üîπ Final Conclusion for Problem 2

This section demonstrates that:

- For Gaussian models, the **MLE is optimal** (minimum variance)  
- The theoretical CRLB matrix is **achievable**  
- Monte-Carlo simulations fully confirm the theoretical derivations  
- The GLS framework naturally generalizes scalar estimation to vector estimation  

Problem 2 therefore extends the ideas of unbiasedness and efficiency to **multi-dimensional parameter spaces**, showing that the same CRLB principles apply.

# üìä Summary Table

| Problem | Estimator | Efficient? | CRLB Achieved? | Notes |
|--------|-----------|------------|----------------|-------|
| 1(a) | Sample Mean | ‚úî Yes | ‚úî Yes | Optimal for i.i.d. Gaussian noise |
| 1(b) | Sample Mean | ‚úò No | ‚úò No | Not MVUE under correlated noise |
| 1(b) | GLS Estimator | ‚úî Yes | ‚úî Yes | Achieves CRLB with correlated covariance |
| 1(c) | Single-Sample Estimator | ‚úò No | ‚úò No | Unbiased but very high variance |
| 1(c) | Sample Mean | ‚úî Yes | ‚úî Yes | Best unbiased estimator in i.i.d. case |
| 2 | MLE / GLS (Vector Case) | ‚úî Yes | ‚úî Yes | Efficient multi-parameter estimator |

---

# Summary Notes

- ‚úî **Efficient** = estimator attains CRLB  
- ‚úò **Not efficient** = estimator is unbiased but has variance above CRLB  
- **GLS** dominates in the presence of correlated noise  
- **Sample Mean** is optimal only for i.i.d. Gaussian noise  
- **MLE/GLS** in Problem 2 achieves CRLB for each component of the parameter vector

# üìå Academic Purpose

This assignment highlights several core ideas in statistical estimation theory:

- Application of classical estimation principles  
- Derivation and validation of unbiased estimators  
- Practical computation and interpretation of the Cram√©r‚ÄìRao Lower Bound (CRLB)  
- Understanding how noise covariance affects estimator performance  
- Efficiency analysis using large-scale Monte-Carlo simulations  
- Estimation of both scalar and multi-dimensional parameter vectors  

These elements collectively demonstrate how theoretical bounds and practical estimators interact in real detection and estimation problems.

---

# üì¨ Contact

**Author:** Ishan Jha  
**Roll Number:** IMT2022562  
**Institution:** IIIT Bangalore  
